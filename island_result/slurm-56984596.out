Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Select jobs to execute...

[Fri Feb 10 14:01:55 2023]
rule fasta_to_res:
    input: /blue/boucher/yu.hong/oneSampTest/ParseIslandData/sni_100.txt
    output: sni_100.20000.out
    jobid: 0
    benchmark: /blue/boucher/yu.hong/oneSampTest/benchmarks/sni_100.benchmark.txt
    wildcards: prefix=sni, loci=100
    resources: mem_mb=1000, disk_mb=1000, tmpdir=/tmp

Traceback (most recent call last):
  File "main.py", line 130, in <module>
    inputFileStatistics.test_stat1()
  File "/blue/boucher/yu.hong/oneSampTest/statistics.py", line 173, in test_stat1
    sampCorrection = 2 / (numloci * (numloci - 1))
ZeroDivisionError: division by zero
[Fri Feb 10 14:02:03 2023]
Error in rule fasta_to_res:
    jobid: 0
    output: sni_100.20000.out
    shell:
        
        module load R/4.1

        python main.py --s 20000 --o /blue/boucher/yu.hong/oneSampTest/ParseIslandData/sni_100.txt > sni_100.20000.out
        
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Removing output files of failed job fasta_to_res since they might be corrupted:
sni_100.20000.out
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
